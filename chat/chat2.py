#!/usr/bin/env python3

""" Use an LLM for text generation with chat functionality. """

import os

os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["USE_TF"] = "0"
os.environ["USE_TORCH"] = "1"

import sys
import logging
from typing import TextIO, AsyncIterator
import asyncio
from collections import deque
import re
from dataclasses import dataclass
import getpass
from threading import Thread

import torch
import transformers  # type: ignore
from argh import arg


from ally import main

__version__ = "0.1.7"

logger = main.get_logger()


default_local_model: str = "default"
default_model: str = str(main.resource(f"models/llm/{default_local_model}"))


class TokenStoppingCriteria(transformers.StoppingCriteria):
    """
    Custom stopping criteria for text generation.

    This stopping criteria stops the generation when any of the specified stop tokens or stop texts are encountered.
    It can handle single or multiple stop tokens or texts.

    **Usage:**
    stopping_criteria = TokenStoppingCriteria(tokenizer, stop_texts=["\n"])
    stopping_criteria = TokenStoppingCriteria(tokenizer, stop_texts=["\n\n"])
    stopping_criteria = TokenStoppingCriteria(tokenizer, stop_texts=["\n", ".", "?", "!"])

    **Args:**
        tokenizer (PreTrainedTokenizer): The tokenizer used for encoding the stop texts.
        stop_texts (list of str): List of strings to stop generation.

    **Returns:**
        bool: `True` if the stop condition is met, `False` otherwise.
    """

    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, stop_texts: list):
        if not isinstance(stop_texts, list):
            raise ValueError("stop_texts must be a list of strings.")
        if not all(isinstance(text, str) for text in stop_texts):
            raise ValueError("All elements in stop_texts must be strings.")

        self.tokenizer = tokenizer
        self.stop_texts = stop_texts
        self.stop_tokens = [
            tokenizer.encode(text, add_special_tokens=False) for text in stop_texts
        ]

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        """
        Checks if the stopping condition is met for any sequence in the batch.

        **Args:**
            input_ids (torch.LongTensor): Tensor of input IDs of shape `(batch_size, sequence_length)`.
            scores (torch.FloatTensor): Prediction scores (not used here).

        **Returns:**
            bool: `True` if the stopping condition is met, `False` otherwise.
        """
        batch_size, sequence_length = input_ids.shape

        # Determine the maximum length of stop tokens
        max_token_len = max(len(t) for t in self.stop_tokens) if self.stop_tokens else 0

        for i in range(batch_size):
            sequence = input_ids[i]
            for stop_token in self.stop_tokens:
                token_len = len(stop_token)
                if token_len > sequence_length:
                    continue  # Sequence too short for this stop token
                if sequence[-token_len:].tolist() == stop_token:
                    logger.debug(f"Stop token found: {stop_token} in {sequence}")
                    return True
            # Check for stop_texts in decoded text
            # Decode only the last few tokens to improve efficiency
            decode_len = min(sequence_length, max_token_len * 2)  # Adjust factor as needed
            decoded_sequence = self.tokenizer.decode(sequence[-decode_len:], skip_special_tokens=True)
            for stop_text in self.stop_texts:
                if decoded_sequence.endswith(stop_text):
                    logger.debug(f"Stop text found: {stop_text!r} in {decoded_sequence!r}")
                    return True
        return False


def debug_tokens(pipeline: transformers.pipeline, text: str):
    """Print the tokens generated by the pipeline."""
    tokenizer = pipeline.tokenizer
    tokens = tokenizer(text, return_tensors="pt")["input_ids"]
    # I want to loop over the tokens, and display each token and its corresponding decoded value.
    for token in tokens[0]:
        logger.debug(f"{token.item()}: {tokenizer.decode([token.item()])}")


def get_pipeline(model: str, stop_texts: list[str] = ["\n", "\n\n"], repetition_penalty: float = 1.0) -> transformers.pipeline:
    """Get the pipeline for the given model."""
    tokenizer = transformers.AutoTokenizer.from_pretrained(model)
    stopping_criteria = TokenStoppingCriteria(tokenizer, stop_texts)
    stopping_criteria_list = transformers.StoppingCriteriaList([stopping_criteria])

    return transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
        # max_length=100,
        max_new_tokens=500,
        truncation=True,
        stopping_criteria=stopping_criteria_list,
        repetition_penalty=repetition_penalty,
        temperature=1.4,
        # temperature=0.7,
        # top_k=50,
        # top_p=0.9,
    )


async def generate_response(
    pipeline, prompt: str, stream: bool = True
) -> AsyncIterator[str]:
    """Generate a response to the given prompt."""
    if stream:
        async for chunk in stream_pipeline_output(pipeline, prompt):
            yield chunk
    else:
        result = await asyncio.to_thread(pipeline, prompt)
        yield result[0]["generated_text"]


async def stream_pipeline_output(pipeline, prompt: str) -> AsyncIterator[str]:
    """Stream the output of the pipeline."""
    streamer = transformers.TextIteratorStreamer(pipeline.tokenizer, skip_prompt=True)
    generation_kwargs = dict(text_inputs=prompt, streamer=streamer)

    thread = Thread(target=pipeline, kwargs=generation_kwargs)
    thread.start()

    for chunk in streamer:
        yield chunk

    thread.join()


@dataclass
class Message:
    """A message with a name and text."""

    name: str
    text: str


def messages_from_lines(lines: list[str]) -> list[Message]:
    """Parse messages from lines."""
    messages: list[Message] = []
    message = None

    def put():
        nonlocal message
        if not message:
            return
        messages.append(message)
        message = None

    for line in lines:
        line = line.rstrip("\n")

        if "\t" in line:
            name, text = line.split("\t", 1)
            # 1. name and text: first line of a new regular message
            if name:
                put()
                message = Message(name, text)
            # 2. indented text only: a continuation of the previous message
            else:
                if message:
                    message.text += "\n" + text
                else:
                    message = Message("", text)
        # 3. blank lines can delimit messages
        elif not line:
            put()
#         # 4. +name, -name: a user joined or left
#         elif line[0] == "+":
#             join(line[1:])
#         elif line[0] == "-":
#             leave(line[1:])
        # 3. unindented text: this is a system message or narration
        else:
            put()
            message = Message("", line)

    put()

    return messages


def message_to_string(message: Message) -> str:
    """Convert a message to a string."""
    if message.name:
        return f"{message.name}: {message.text}"
    else:
        return message.text


def read_chat_history(filename: str) -> list[Message]:
    """Read and return the chat history from the given file."""
    if not os.path.exists(filename):
        return []

    with open(filename, "r") as f:
        lines = f.readlines()

    return messages_from_lines(lines)


def display_chat_history(history: list[Message], put_func):
    """Display the chat history."""
    for message in history:
        put_func(message_to_string(message) + "\n\n")


def write_message(f: TextIO, message: Message):
    """Write a message to the file."""
    # prepend a tab to each line of the message
    text = message.text
    if message.name:
        text = re.sub(r"^", "\t", text, flags=re.MULTILINE)
    f.write(f"{message.name or ''}\t{message.text}\n\n")


def log_message(filename: str, message: Message):
    """Append a message to the log file."""
    with open(filename, "a") as f:
        write_message(f, message)


def write_chat_history(filename: str, history: list[Message], mode: str = "w"):
    """Write the chat history to the given file."""
    if mode == "w":
        main.backup(filename)
    with open(filename, mode) as f:
        for message in history:
            write_message(f, message)


@arg("chat_file", help="File to save chat history")
@arg("--model", help="model")
@arg("--stream", help="Enable streaming output")
@arg("--context", help="Number of previous messages to include for context")
@arg("--ai_name", help="Name of the AI assistant")
@arg("--user_name", help="Name of the user", default=None)
@arg("--first", help="AI speaks first", default=False)
@arg("--repetition_penalty", help="Repetition penalty for text generation")
async def chat_with_ai(
    chat_file: str,
    istream: TextIO = sys.stdin,
    ostream: TextIO = sys.stdout,
    model: str | None = None,
    stream: bool = True,
    context: int = 1000,
    ai_name: str = "Ally",
    user_name: str | None = None,
    first: bool = False,
    repetition_penalty: float = 1.0,
) -> None:
    """
    Chat with a local LLM model.
    """
    if not model:
        model = default_model

    if not user_name:
        user_name = getpass.getuser().title()

    get, put = main.io(istream, ostream)

    stop_texts = [f"\n{ai_name}:", f"\n{user_name}:"]

    pipeline = get_pipeline(model, stop_texts, repetition_penalty)

    message_history: list[Message] = []

    pipeline = get_pipeline(model, stop_texts)

    message_history: list[Message] = []

    if chat_file:
        message_history = read_chat_history(chat_file)
        display_chat_history(message_history, put)

    # NOTE: get and put are not async, yet
    # No big deal for this app.

    def erase_messages(count: int):
        del message_history[-count:]
        write_chat_history(chat_file, message_history)
        logger.info(f"erased {count} messages")

    def edit_previous_message():
        if not message_history:
            logger.info("no previous message to edit")
            return
        message = message_history[-1]
        message = Message(ai_name, get(f"{ai_name}: ", placeholder=message.text))
        erase_messages(1)
        log_message(chat_file, message)

    async def user_turn():
        message = Message(user_name, get(f"{user_name}: "))
        put()
        if message.text is None:
            raise EOFError()
        # TODO a Messages class could handle this
        message.text = message.text.strip()
        if re.match(r"\x08+$", message.text):   # C-V C-H: erase (can do multiple)
            erase_messages(len(message.text))
            return await user_turn()
        if re.match(r"\x0d$", message.text):    # C-V enter: skip
            logger.info("skipping turn")
            return None
        if re.match(r"\x05$", message.text):    # C-V C-E: edit
            logger.info("editing previous message")
            edit_previous_message()
            return await user_turn()
        if re.match(r"\x01$", message.text):    # C-V C-A: again
            logger.info("again")
            erase_messages(1)
            return None
        if chat_file:
            log_message(chat_file, message)
        message_history.append(message)
        return message

    async def ai_turn():
        context_prompt = "\n\n".join(map(message_to_string, message_history[-context:])) + f"\n\n{ai_name}:"
        put(f"{ai_name}:", end="")
        message = Message(ai_name, "")
        try:
            async for chunk in generate_response(pipeline, context_prompt, stream=stream):
                text2 = message.text + chunk
                if not any(text2.endswith(stopper) for stopper in stop_texts):
                    message.text = text2
                    put(chunk, end="", flush=True)
        except KeyboardInterrupt:
            logger.warning("Interrupted")
        # TODO give the AI the ability to stop the conversation
        if not message.text.endswith("\n"):
            put()
        if not message.text.endswith("\n\n"):
            put()
        # TODO dup code, see Messages class idea above
        message.text = message.text.strip()
        if chat_file:
            log_message(chat_file, message)
        message_history.append(message)
        return message

    # Main loop
    if first:
        await ai_turn()
    try:
        while True:
            await user_turn()
            await ai_turn()
    except EOFError:
        pass

    # TODO With username stop texts, we can tell who the AI expects to reply, which could be useful!
    # It could even call out to someone who isn't in the chat yet, or an imaginary friend, or whatever.


if __name__ == "__main__":
    main.run(chat_with_ai)
