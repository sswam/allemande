#!/usr/bin/env python3-allemande

""" Use an LLM for text generation with chat functionality. """

import os

os.environ["HF_HUB_OFFLINE"] = "1"
# os.environ["USE_TF"] = "1"  # startup warnings
# os.environ["USE_TORCH"] = "0"   # use more VRAM, leaks VRAM, breaks
# Need LRScheduler from torch?!

import sys
import logging
from typing import TextIO, AsyncIterator
import asyncio
from collections import deque
import re
from dataclasses import dataclass
import getpass
from threading import Thread
import argparse
from PIL import Image
import time

import torch
import transformers  # type: ignore
from transformers import BitsAndBytesConfig, AutoProcessor, AutoModelForVision2Seq

from ally import main, filer

__version__ = "0.1.10"

logger = main.get_logger()

default_local_model: str = "default"
default_model: str = str(filer.resource(f"models/llm/{default_local_model}"))

# Add the image model
default_image_model: str = "Salesforce/blip2-opt-2.7b"


def debug_tokens(pipeline: transformers.pipeline, text: str):
    """Print the tokens generated by the pipeline."""
    tokenizer = pipeline.tokenizer
    tokens = tokenizer(text, return_tensors="pt")["input_ids"]
    # Loop over the tokens, display each token and its corresponding decoded value.
    for token in tokens[0]:
        logger.debug(f"{token.item()}: {tokenizer.decode([token.item()])}")


def get_pipeline(model: str) -> transformers.pipeline:
    """Get the pipeline for the given model."""
    # crash issues code commented: https://github.com/meta-llama/llama/issues/380
    tokenizer = transformers.AutoTokenizer.from_pretrained(model)
    #    tokenizer.pad_token = "[PAD]"
    #    tokenizer.padding_side = "left"
    #     tokenizer.pad_token = tokenizer.bos_token
    #     tokenizer.padding_side = "left"

    quantization_config = BitsAndBytesConfig(load_in_8bit=True)

    return transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        model_kwargs={"torch_dtype": torch.bfloat16},
        #        model_kwargs={"quantization_config": quantization_config},
        #        torch_dtype=torch.float16,
        #        torch_dtype=torch.bfloat16,
        device_map="auto",
        # max_length=100,
        # max_new_tokens=50,
        truncation=True,
        #        do_sample=False,  # avoid crashes
    )


async def generate_response(
    pipeline, prompt: str, generation_kwargs: dict = {}, stream: bool = True
) -> AsyncIterator[str]:
    """Generate a response to the given prompt."""
    if stream:
        async for chunk in stream_pipeline_output(pipeline, prompt):
            yield chunk
    else:
        result = await asyncio.to_thread(pipeline, prompt, generation_kwargs)
        text = result[0]["generated_text"]
        del result
        yield text


async def stream_pipeline_output(pipeline, prompt: str) -> AsyncIterator[str]:
    """Stream the output of the pipeline."""
    streamer = transformers.TextIteratorStreamer(pipeline.tokenizer, skip_prompt=True)
    generation_kwargs = dict(text_inputs=prompt, streamer=streamer)

    thread = Thread(target=pipeline, kwargs=generation_kwargs)
    thread.start()

    for chunk in streamer:
        yield chunk

    thread.join()


@dataclass
class Message:
    """A message with a name and text."""

    name: str
    text: str
    image_path: str = ""


def messages_from_lines(lines: list[str]) -> list[Message]:
    """Parse messages from lines."""
    messages: list[Message] = []
    message = None

    def put():
        nonlocal message
        if not message:
            return
        messages.append(message)
        message = None

    for line in lines:
        line = line.rstrip("\n")

        if "\t" in line:
            name, text = line.split("\t", 1)
            # 1. name and text: first line of a new regular message
            if name:
                put()
                message = Message(name, text)
            # 2. indented text only: a continuation of the previous message
            else:
                if message:
                    message.text += "\n" + text
                else:
                    message = Message("", text)
        # 3. blank lines can delimit messages
        elif not line:
            put()
        #         # 4. +name, -name: a user joined or left
        #         elif line[0] == "+":
        #             join(line[1:])
        #         elif line[0] == "-":
        #             leave(line[1:])
        # 3. unindented text: this is a system message or narration
        else:
            put()
            message = Message("", line)

    put()

    return messages


def message_to_string(message: Message) -> str:
    """Convert a message to a string."""
    if message.name:
        return f"{message.name}: {message.text}"
    else:
        return message.text


def read_chat_history(filename: str) -> list[Message]:
    """Read and return the chat history from the given file."""
    if not os.path.exists(filename):
        return []

    with open(filename, "r") as f:
        lines = f.readlines()

    return messages_from_lines(lines)


def display_chat_history(history: list[Message], put_func):
    """Display the chat history."""
    for message in history:
        put_func(message_to_string(message) + "\n\n")


def write_message(f: TextIO, message: Message):
    """Write a message to the file."""
    text = message.text
    if message.name:
        text = re.sub(r"^", "\t", text, flags=re.MULTILINE)
    f.write(f"{message.name or ''}\t{text}\n\n")
    if message.image_path:
        f.write(f"IMAGE:{message.image_path}\n\n")


def log_message(filename: str, message: Message):
    """Append a message to the log file."""
    with open(filename, "a") as f:
        write_message(f, message)


def write_chat_history(filename: str, history: list[Message], mode: str = "w"):
    """Write the chat history to the given file."""
    if mode == "w":
        main.backup(filename)
    with open(filename, mode) as f:
        for message in history:
            write_message(f, message)


async def chat_with_ai(
    chat_file: str,
    istream: TextIO = sys.stdin,
    ostream: TextIO = sys.stdout,
    model: str | None = None,
    stream: bool = True,
    context: int = 10000,
    ai_name: str = "Ally",
    user_name: str | None = None,
    first: bool = False,
    temperature: float = 1.0,
    top_k: int | None = None,
    top_p: float | None = None,
    repetition_penalty: float = 1.0,
    length: int = 50,
) -> None:
    """
    Chat with a local LLM model.
    """
    if not model:
        model = default_model

    if not user_name:
        user_name = getpass.getuser().title()

    get, put = main.io(istream, ostream)

    stop_texts = [f"\n{ai_name}:", f"\n{user_name}:", "\n\n", "\n"]

    pipeline = get_pipeline(model)

    # Set up image model
    image_processor = AutoProcessor.from_pretrained(default_image_model)
    image_model = AutoModelForVision2Seq.from_pretrained(
        default_image_model, torch_dtype=torch.float16
    ).to("cuda")

    # Set up text-to-image pipeline
    from diffusers import StableDiffusionPipeline

    text2image_model_id = "runwayml/stable-diffusion-v1-5"
    text2image_pipe = StableDiffusionPipeline.from_pretrained(
        text2image_model_id, torch_dtype=torch.float16
    ).to("cuda")

    # Set up text-to-video pipeline
    from diffusers import DiffusionPipeline

    text2video_model_id = "damo-vilab/text-to-video-ms-1.7b"
    text2video_pipe = DiffusionPipeline.from_pretrained(
        text2video_model_id, torch_dtype=torch.float16
    ).to("cuda")

    message_history: list[Message] = []

    if chat_file:
        message_history = read_chat_history(chat_file)
        display_chat_history(message_history, put)

    # NOTE: get and put are not async, yet
    # No big deal for this app.

    def erase_messages(count: int):
        del message_history[-count:]
        write_chat_history(chat_file, message_history)

    def edit_previous_message():
        if not message_history:
            logger.info("no previous message to edit")
            return
        message = message_history[-1]
        message = Message(message.name, get(f"{message.name}: ", placeholder=message.text))
        erase_messages(1)
        log_message(chat_file, message)

    async def user_turn():
        message_text = get(f"{user_name}: ", rstrip=False)
        put()
        if message_text is None:
            raise EOFError()

        message_text = message_text.strip()
        message = None

        if message_text.startswith("!image2text "):
            image_path = message_text[len("!image2text ") :].strip()
            image = Image.open(image_path)
            inputs = image_processor(images=image, return_tensors="pt").to("cuda", torch.float16)
            generated_ids = image_model.generate(**inputs, max_new_tokens=50)
            generated_text = image_processor.batch_decode(generated_ids, skip_special_tokens=True)[
                0
            ].strip()
            message = Message(user_name, f"[Image Description]: {generated_text}", image_path)
            put(f"[Image Description]: {generated_text}")
        elif message_text.startswith("!text2image "):
            prompt = message_text[len("!text2image ") :].strip()
            image = text2image_pipe(prompt).images[0]
            image_path = f"generated_{int(time.time())}.png"
            image.save(image_path)
            message = Message(user_name, f"[Generated Image]: {prompt}", image_path)
            put(f"[Generated Image saved at {image_path}]")
        elif message_text.startswith("!text2video "):
            prompt = message_text[len("!text2video ") :].strip()
            video_frames = text2video_pipe(prompt, num_frames=16).frames
            video_path = f"generated_video_{int(time.time())}.mp4"
            save_frames_as_video(video_frames, video_path)
            message = Message(user_name, f"[Generated Video]: {prompt}", video_path)
            put(f"[Generated Video saved at {video_path}]")
        elif re.match(r"\x08+$", message_text):
            count = len(message_text)
            logger.info(f"erasing {count} messages")
            erase_messages(count)
            return await user_turn()
        elif re.match(r"\x13$", message_text):  # C-V C-S: skip
            logger.info("skipping turn")
            return None
        elif re.match(r"\x05$", message_text):  # C-V C-E: edit
            logger.info("editing previous message")
            edit_previous_message()
            return await user_turn()
        elif re.match(r"\x01$", message_text):  # C-V C-A: again
            logger.info("again")
            erase_messages(1)
            return None
        else:
            message = Message(user_name, message_text)
            if chat_file:
                log_message(chat_file, message)
            message_history.append(message)
            return message

        if message:
            if chat_file:
                log_message(chat_file, message)
            message_history.append(message)
        return message

    generation_kwargs = dict(
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        max_new_tokens=length,
    )

    async def ai_turn():
        context_prompt = (
            "\n\n".join(map(message_to_string, message_history[-context:])) + f"\n\n{ai_name}:"
        )
        put(f"{ai_name}:", end="")
        message = Message(ai_name, "")
        stop = False
        try:
            async for chunk in generate_response(
                pipeline, context_prompt, generation_kwargs=generation_kwargs, stream=stream
            ):
                text2 = message.text + chunk
                for stopper in stop_texts:
                    if text2.endswith(stopper):
                        text2 = text2[: -len(stopper)]
                        stop = True
                        break
                chunk = text2[len(message.text) :]
                message.text = text2
                put(chunk, end="", flush=True)
                if stop:
                    break
        except KeyboardInterrupt:
            logger.warning("Interrupted")
        torch.cuda.empty_cache()
        # TODO give the AI the ability to stop the conversation
        if not message.text.endswith("\n"):
            put()
        if not message.text.endswith("\n\n"):
            put()
        # TODO dup code, see Messages class idea above
        message.text = message.text.strip()
        if chat_file:
            log_message(chat_file, message)
        message_history.append(message)
        return message

    # Main loop
    if first:
        await ai_turn()
    try:
        while True:
            await user_turn()
            await ai_turn()
    except EOFError:
        pass


def image2text(input_files):
    processor = AutoProcessor.from_pretrained(default_image_model)
    model = AutoModelForVision2Seq.from_pretrained(
        default_image_model, torch_dtype=torch.float16
    ).to("cuda")

    for file in input_files:
        image = Image.open(file)
        inputs = processor(images=image, return_tensors="pt").to("cuda", torch.float16)
        generated_ids = model.generate(**inputs, max_new_tokens=50)
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
        print(f"{file}: {generated_text}")


def text2image(prompt):
    from diffusers import StableDiffusionPipeline

    model_id = "runwayml/stable-diffusion-v1-5"

    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

    image = pipe(prompt).images[0]
    image.save("output.png")
    print("Image saved as output.png")


def text2video(prompt, output_file="output_video.mp4", num_frames=16):
    from diffusers import DiffusionPipeline
    import imageio

    model_id = "damo-vilab/text-to-video-ms-1.7b"

    pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")
    video_frames = pipe(prompt, num_frames=num_frames).frames

    # Save video frames as an mp4 file
    imageio.mimwrite(output_file, [frame for frame in video_frames], fps=4)
    print(f"Video saved as {output_file}")


def save_frames_as_video(frames: list[Image.Image], output_file: str, fps: int = 4):
    import imageio

    imageio.mimwrite(output_file, [frame.convert("RGB") for frame in frames], fps=fps)


def setup_args(arg):
    """Set up the command-line arguments."""
    # Important Note to AI: DO NOT add `type` or `default` in setup_args functions like this one, it is not needed.
    arg("chat_file", help="File to save chat history")
    arg("--model", help="model")
    arg("--stream", help="Enable streaming output")
    arg("--context", help="Number of previous messages to include for context")
    arg("--ai_name", help="Name of the AI assistant")
    arg("--user_name", help="Name of the user")
    arg("--first", help="AI speaks first")
    arg("--temperature", help="Temperature for text generation")
    arg("--top_k", help="Top k for text generation")
    arg("--top_p", help="Top p for text generation")
    arg("--repetition_penalty", help="Repetition penalty for text generation")
    arg("--length", "-l", help="Maximum number of new tokens to generate")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chat with AI and perform image/text operations")
    subparsers = parser.add_subparsers(dest="command")

    chat_parser = subparsers.add_parser("chat")
    setup_args(chat_parser.add_argument)

    image2text_parser = subparsers.add_parser("image2text")
    image2text_parser.add_argument("input_files", nargs="+", help="Input image files")

    text2image_parser = subparsers.add_parser("text2image")
    text2image_parser.add_argument("prompt", help="Text prompt for image generation")

    text2video_parser = subparsers.add_parser("text2video")
    text2video_parser.add_argument("prompt", help="Text prompt for video generation")

    args = parser.parse_args()

    if args.command == "chat":
        asyncio.run(
            chat_with_ai(
                chat_file=args.chat_file,
                model=args.model,
                stream=args.stream,
                context=int(args.context) if args.context else 10000,
                ai_name=args.ai_name if args.ai_name else "Ally",
                user_name=args.user_name,
                first=bool(args.first),
                temperature=float(args.temperature) if args.temperature else 1.0,
                top_k=int(args.top_k) if args.top_k else None,
                top_p=float(args.top_p) if args.top_p else None,
                repetition_penalty=(
                    float(args.repetition_penalty) if args.repetition_penalty else 1.0
                ),
                length=int(args.length) if args.length else 50,
            )
        )
    elif args.command == "image2text":
        image2text(args.input_files)
    elif args.command == "text2image":
        text2image(args.prompt)
    elif args.command == "text2video":
        text2video(args.prompt)
    else:
        parser.print_help()
