Devlog: March 27 - April 2, 2023

After forking `point-alpaca` last week and christening it (for now), this week was about getting my hands dirty and turning a generic AI repo into a set of actually useful, interconnected tools. The theme was clear: build a multi-modal Swiss Army knife. No grand architecture, just a rapid succession of scripts that do cool, useful shit.

Here's the summary of the damage:

*   **Created a video-to-flashcard pipeline.** This is the centerpiece of the week's work, combining speech-to-text and a large language model to automate study material creation.
*   **Integrated GPT-4 and Whisper-large.** Moved straight to using some more powerful models for better results.
*   **Added web search and OCR capabilities.** The system is no longer just a brain in a jar; it can now read the web and extract text from images.
*   **Built out a basic text-to-speech (TTS) output.** The scripts can now talk back, with options for different voices.

---

### From Video to Knowledge with GPT-4

The main event this week was building a workflow to to turn video content into flashcards. The process is straightforward but powerful. You feed it a video, it uses Whisper to get a full transcript, and then shoves that transcript into GPT-4 with a prompt to distill it into a set of question/answer flashcards.

This isn't just a gimmick. It's a genuinely useful tool. Imagine feeding it a two-hour university lecture and getting back a complete set of study notes without having to type a single word yourself. Naturally, once it was working, I had to do a demo, which led to the inevitable "fixes during demo" commit. Such is the cycle of life!

### Giving the AI Eyes and Ears

A language model needs access to the outside world. To that end, I bolted on a couple of new senses.

First, a web search script. This allows the system to pull in current information, breaking it out of the static "knowledge cutoff" prison that plagues most off-the-shelf models.

Second, an OCR script. Now it can read text from images. This opens up a ton of possibilities, from processing screenshots of articles to pulling text from slides in a video that Whisper might miss. The system is starting to see the world, not just hear it.

### Finding Its Voice

What good is an AI that does all this work if you still have to read all the output? To close the loop, I added text-to-speech. It's basic for now. I also added the ability to resume speaking from an existing audio file, so you don't have to regenerate the whole thing every time.

This week wasn't about building a polished app. It was a flurry of foundational, practical scripting. It was a messy, iterative process of building, demoing, and fixing. But the toolbox is starting to take shape, and its components are already pretty capable.
