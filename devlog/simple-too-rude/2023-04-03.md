### **This Week: More Tools, More Senses, Less Annoyance**

The focus this week was on making the AI chat experience less of a one-trick pony. An AI that only does text is boring. We're adding a whole suite of tools for wrangling text, reviving the voice chat system, giving the AI eyes to see images, and making it easier to swap the AI's brain mid-conversation. The goal is a flexible, powerful platform, not another locked-down web app.

**Wrangling Text is 90% of the Work**
Feeding clean data to an LLM is critical, and its output is often a mess. To deal with this, I've built up a small army of command-line text processing tools. Instead of one monolithic script, we have a proper toolkit in the `text/` directory for everything from fixing blank lines to converting data formats.

Some of the more interesting additions:
*   **AI fixing text for AI:** A new script, `split-long-sentences-ai.py`, uses an LLM to break down ridiculously long sentences into something more coherent.
*   **Web scraping helper:** `html2selectors.py` is a neat utility that pulls CSS selectors from HTML, making it easier to extract content from web pages.
*   **For the C-lovers (and Python-haters):** `brace.py` converts Python's indentation into proper C-style braces, because some of us prefer brackets.
*   **Lossless diffing:** `words-split.py` and `words-join.py` can break text into a word-per-line format and perfectly rejoin it, which is a godsend for version control and patching.

**Making the AI Talk Back (and Listen)**
Voice chat is back online. The system is a bit of a Frankenstein's monster, running three main components in separate terminals:
1.  **The Brain (`barbarella.py`):** The core LLM that thinks.
2.  **The Ears (`mike.py`):** Transcribes speech-to-text.
3.  **The Mouth (`speak.py`):** Reads the AI's response out loud (text-to-speech).

A new `toggle-mic.sh` script provides a much-needed hardware mute/unmute function, because nobody wants a hot mic all the time.

**Now the AI Can See**
The project is no longer blind. A couple of new vision tools allow for some serious image analysis:
*   **Segment Anything Model (SAM):** A script (`vision/amg`) uses SAM to generate masks for any object in an image. Think of it as a super-powered magic wand tool that can automatically cut things out of a picture.
*   **Image-to-Text:** Using a CLIP-based model, `vision/image2text.py` can look at an image and generate a surprisingly detailed text description of what it sees.

**Don't Like the AI's Answer? Swap Its Brain.**
A new user script for the browser lets you switch the ChatGPT model (e.g., from GPT-3.5 to GPT-4) right in the middle of a conversation. It works by adding a simple dropdown to the UI and intercepting the API calls to change the model parameter on the fly. It's a simple hack, but it gives the user back a little control.

**General Housekeeping**
Also did some boring but necessary cleanup: fixed a file-watching bug, moved some scripts around, and added a clever `huggingface-get` script to download models without duplicating them all over the hard drive, because disk space isn't free.
