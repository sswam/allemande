### **Teaching AI to read the web, without the garbage.**
Back from a two-week break. Dove straight into making Ally Chat's web processing smarter and had some thoughts on how we're training LLMs all wrong.

**Making AI dumber to make it smarter.**
LLMs are bloated with useless trivia. Why force them to memorize the entire internet when they could just be good at reasoning? My idea is simple: treat the AI like it's taking an open-book exam. Give it the knowledge it needs in the prompt, and train it to use that information to solve problems. This frees up its 'brain' to focus on actual intelligence instead of being a glorified search index.

**Scraping the crap from HTML.**
Most of the web is crap. To feed a clean article to an AI, you have to cut out the ads, navigation bars, footers, and other junk. I improved my `pandoc-dump` script to do just that. It now intelligently strips out garbage like Wikipedia's "References" section and can zero in on the main content by looking for `<main>` or `<article>` tags. Clean input, better output.

**Testing how the AIs see text.**
Once the text is clean, you have to see how different AIs will 'see' it. I updated my test script to analyze web pages using both Claude's and ChatGPT's tokenization methodsâ€”basically, how they chop up words into pieces. This helps me measure how efficiently each model processes the same information.

**A bug that did nothing.**
Also squashed a dumb bug. A line of code meant to filter out junk image data had a typo and was doing absolutely nothing. Commented it out for now until I can fix it properly. Classic.
