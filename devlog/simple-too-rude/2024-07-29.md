### **Taming LLMs and Tidying Up the Toolbox**

After a week off, I got back to work on the plumbing. This week was about making our tools smarter and less of a pain in the ass to use, especially when it comes to wrangling LLMs from the command line.

### **Making AI on the Command Line Suck Less**

Working with Large Language Models from a terminal can be clunky. I added a few scripts to streamline the whole process.

*   **See what you just did:** The new `llm-last` script lets you quickly view your most recent prompts and the AI's answers. No more digging through log files just to remember what you asked.
*   **Get straight to the point:** I created `llm/proc` and `llm/que` as shortcuts to get concise answers from the AI, stripping out the usual "As a large language model..." boilerplate. One is for processing tasks, the other for quick questions.
*   **Sane logging:** I split the logs so that prompts and answers go into separate `prompt.*` and `answer.*` files. Debugging is now about 10x less annoying.

### **Little Victories in Scripting**

A few other quality-of-life improvements:

*   **Ripping Subtitles from Videos:** Built a new `video-subtitles` script. It uses `ffmpeg` to pull subtitles out of video files into a clean text format, which is perfect for analysis or feeding to an AI.
*   **Smarter Syncing:** My script for syncing code repositories now pulls from all sources *before* it pushes any changes. A simple reordering of commands that prevents a world of potential headaches and merge conflicts.
*   **Cleaner Git History:** The `llm-git-commit` script, which uses an AI to write commit messages, was getting a little too enthusiastic and including entire code blocks in the summary. I taught it to strip that noise out for a more readable history.
