title: Building with extended thinking - Anthropic
From: https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking

# Building with extended thinking

Extended thinking gives Claude 3.7 Sonnet enhanced reasoning capabilities for complex tasks, while also providing transparency into its step-by-step thought process before it delivers its final answer.

## [​](#how-extended-thinking-works) How extended thinking works

When extended thinking is turned on, Claude creates thinking content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.

The API response will include both thinking and text content blocks.

In multi-turn conversations, only thinking blocks associated with a tool use session or assistant turn in the last message position are visible to Claude and are billed as input tokens; thinking blocks associated with earlier assistant messages are [not visible](/en/docs/build-with-claude/context-windows#the-context-window-with-extended-thinking) to Claude during sampling and do not get billed as input tokens.

## [​](#implementing-extended-thinking) Implementing extended thinking

Add the thinking parameter and a specified token budget to use for extended thinking to your API request.

The budget_tokens parameter determines the maximum number of tokens Claude is allowed use for its internal reasoning process. Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32K.

Your budget_tokens must always be less than the max_tokens specified.

```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=[{
        "role": "user",
        "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
)
```

print(response)

The API response will include both thinking and text content blocks:

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "To approach this, let's think about what we know about prime numbers...",
      "signature": "zbbJhbGciOiJFU8zI1NiIsImtakcjsu38219c0.eyJoYXNoIjoiYWJjMTIzIiwiaWFxxxjoxNjE0NTM0NTY3fQ...."
    },
    {
      "type": "text",
      "text": "Yes, there are infinitely many prime numbers such that..."
    }
  ]
}
```

## [​](#understanding-thinking-blocks) Understanding thinking blocks

Thinking blocks represent Claude's internal thought process. In order to allow Claude to work through problems with minimal internal restrictions while maintaining our safety standards and our stateless APIs, we have implemented the following:

- Thinking blocks contain a signature field. This field holds a cryptographic token which verifies that the thinking block was generated by Claude, and is verified when thinking blocks are passed back to the API. When streaming responses, the signature is added via a signature_delta inside a content_block_delta event just before the content_block_stop event. It is only strictly necessary to send back thinking blocks when using [tool use with extended thinking](/#preserving-thinking-blocks-during-tool-use) . Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back.
- Occasionally Claude's internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the thinking block and return it to you as a redacted_thinking block. These redacted thinking blocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.

Here's an example showing both normal and redacted thinking blocks:

```python
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpP..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```

Seeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.

If you need to test redacted thinking handling in your application, you can use this special test string as your prompt: ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB

When passing thinking and redacted_thinking blocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn.

This is critical for maintaining the model's reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the [Preserving thinking blocks](/_sites/docs.anthropic.com/en/docs/build-with-claude/extended-thinking#preserving-thinking-blocks) section below.

Example: Working with redacted thinking blocks

This example demonstrates how to handle redacted_thinking blocks that may appear in responses when Claude's internal reasoning contains content flagged by safety systems:

```python
import anthropic

client = anthropic.Anthropic()

# Using a special prompt that triggers redacted thinking (for demonstration purposes only)
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
)

# Identify redacted thinking blocks
has_redacted_thinking = any(
    block.type == "redacted_thinking" for block in response.content
)

if has_redacted_thinking:
    print("Response contains redacted thinking blocks")
    # These blocks are still usable in subsequent requests

    # Extract all blocks (both redacted and non-redacted)
    all_thinking_blocks = [
        block for block in response.content
        if block.type in ["thinking", "redacted_thinking"]
    ]

    # When passing to subsequent requests, include all blocks without modification
    # This preserves the integrity of Claude's reasoning

    print(f"Found {len(all_thinking_blocks)} thinking blocks total")
    print(f"These blocks are still billable as output tokens")
```

### [​](#suggestions-for-handling-redacted-thinking-in-production) Suggestions for handling redacted thinking in production

When building customer-facing applications that use extended thinking:

- Be aware that redacted thinking blocks contain encrypted content that isn't human-readable
- Consider providing a simple explanation like: "Some of Claude's internal reasoning has been automatically encrypted for safety reasons. This doesn't affect the quality of responses."
- If showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks
- Be transparent that using extended thinking features may occasionally result in some reasoning being encrypted
- Implement appropriate error handling to gracefully manage redacted thinking without breaking your UI

## [​](#streaming-extended-thinking) Streaming extended thinking

When streaming is enabled, you'll receive thinking content via thinking_delta events. Here's how to handle streaming with thinking:

```python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=[{
        "role": "user",
        "content": "What is 27 * 453?"
    }]
) as stream:
    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                print(f"Thinking: {event.delta.thinking}", end="", flush=True)
            elif event.delta.type == "text_delta":
                print(f"Response: {event.delta.text}", end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

Example streaming output:

```
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-3-7-sonnet-20250219", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```

**About streaming behavior with thinking**

When using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.

The streaming system needs to process content in batches for optimal performance, which can result in this "chunky" delivery pattern. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.

redacted_thinking blocks will not have any deltas associated and will be sent as a single event.

Example: Streaming with redacted thinking

import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=[{
        "role": "user",
        "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
    }]
) as stream:
    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                print(f"Thinking: {event.delta.thinking}", end="", flush=True)
            elif event.delta.type == "text_delta":
                print(f"Response: {event.delta.text}", end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")

This will output:

event: message_start
data: {"type":"message_start","message":{"id":"msg_018J5iQyrGb5Xgy5CWx3iQFB","type":"message","role":"assistant","model":"claude-3-7-sonnet-20250219","content":[],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":92,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":3}}       }

event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"redacted_thinking","data":"EvEBCoYBGAIiQAqN5Z4LumxzafxD2yf2zW+hVm/G2/Am05ChRXkU1Xe2wQLPLo0wnmoaVJI1WTkLpRYJAIz2UjzHblLwkJ59xeAqQNr5EWqMZkOr8yNcpbCO5PssXiUvEjhoaC0IN3qyhE3vumOOS9Qd0Ku4AYTgu8VjP4C6IJHnkuIexa0VrU/cFbISDJjPWOWQlyAx4y5FCRoMk55jLUCR8KCZrKrzIjDR8S3F/pCWlz/JA5RN0uprpWAI75HjgcY2NJkPX3sEC0Ew6fl6YEISNk1XsmzWtj4qGArQlCfAW9l8SDiKbXm0UZ4hQhh2ruPbaw=="}  }

event: ping
data: {"type": "ping"}

event: content_block_stop
data: {"type":"content_block_stop","index":0         }

event: content_block_start
data: {"type":"content_block_start","index":1,"content_block":{"type":"redacted_thinking","data":"EvMBCoYBGAIiQKZ6LAz+dCNWxvz0dmjI0gfqEInA9MLVAtFJTpolzOaIbUs28xuKyXVzEQsPWPvP12gN+hxVJ4mYzWT8DCIAxXIqQHwQZcGASLMxWCfHrUlYfFq0vF8IGhRgQKxpj1zxouNLuKdhpZrcHF9vKODIPCPW8EWD13aI6t+exz/UboOs/ZMSDA8tVDp4vkOEUc7sGBoMbiRhGYMqcmAOhb3nIjC/lewBt2l9P+VpJkV78YQ3LhvNh/q3KfsbGuJ2U+lIMPGf9wnzrRC/6xqdsHPe1B0qGozBPKnbBifhyb7xYyWcEWoi/qW9OdoFl1/w"}            }

event: content_block_stop
data: {"type":"content_block_stop","index":1             }

event: content_block_start
data: {"type":"content_block_start","index":2,"content_block":{"type":"redacted_thinking","data":"Eu8BCoYBGAIiQCXtUNB4QyT//Zww832Q+xjJ0oa7/PQZr74OvbS1+a7cRNywZfYMYGGte3RXXTMa6I0bFJOMmXXckcbLxR/L+msqQLhKGx9Bt2FnLpo7bp/PdMQBDDCo+jkbOctnxBQrHCuYbu33o30qPCh73AZ8O1xXXEZfzfLC0L6RoHzLxQSHN5gSDAxGSY7Ifg073BaUYBoMSWHLVrmZrydEfc7SIjAF1R+fYlyVPFwS4Sac/Dw9caskXNF/p+Yn7RNaW9+v/jL03qsqqvemuqRGltSBfZcqFrowQipxo/ftIkEC47Ua64RzSBIe27E="}   }

event: content_block_stop
data: {"type":"content_block_stop","index":2              }

event: content_block_start
data: {"type":"content_block_start","index":3,"content_block":{"type":"redacted_thinking","data":"Eu8BCoYBGAIiQEgE6WUvQO3d6fPpY3OaA95soqeWgZv/Nyi0X6iywTb5KqvUn9NxWySiZwSFZb+4S8ymtHRO4OBKA7eRWEXcBuQqQNudvV6YSFH5ErwaDME0HaEjtHcuy8SslL6RhLwhEJKGpYCzq7zWupcMBB1g57sR8vh/JwGjr7D9sfX9jmM7EsESDEatCbzVVczyZ0TERRoMenFOToj2qn0Xmh1LIjA1WgxaMqiHhb5T4k/++UCKNMH2SEseLzTlR7uIz20qZUXDWtoVck6wc+x7lSWRKXQqFiLoTO1oG0I/lbPz1n2FgC3MH7683FU="}     }

// Additional events...

event: content_block_start
data: {"type":"content_block_start","index":58,"content_block":{"type":"redacted_thinking","data":"EuoBCoYBGAIiQJ/SxkPAgqxhKok29YrpJHRUJ0OT8ahCHKAwyhmRuUhtdmDX9+mn4gDzKNv3fVpQdB01zEPMzNY3QuTCd+1bdtEqQK6JuKHqdndbwpr81oVWb4wxd1GqF/7Jkw74IlQa27oobX+KuRkopr9Dllt/RDe7Se0sI1IkU7tJIAQCoP46OAwSDF51P09q67xhHlQ3ihoM2aOVlkghq/X0w8NlIjBMNvXYNbjhyrOcIg6kPFn2ed/KK7Cm5prYAtXCwkb4Wr5tUSoSHu9T5hKdJRbr6WsqEc7Lle7FULqMLZGkhqXyc3BA"}        }

event: content_block_stop
data: {"type":"content_block_stop","index":58            }

event: content_block_start
data: {"type":"content_block_start","index":59,"content_block":{"type":"text","text":""}              }

event: content_block_delta
data: {"type":"content_block_delta","index":59,"delta":{"type":"text_delta","text":"I'm"}        }

event: content_block_delta
data: {"type":"content_block_delta","index":59,"delta":{"type":"text_delta","text":" not"}     }

event: content_block_delta
data: {"type":"content_block_delta","index":59,"delta":{"type":"text_delta","text":" sure"}    }

// Additional text deltas...

event: content_block_delta
data: {"type":"content_block_delta","index":59,"delta":{"type":"text_delta","text":" me know what you'"}          }

event: content_block_delta
data: {"type":"content_block_delta","index":59,"delta":{"type":"text_delta","text":"d like assistance with."}   }

event: content_block_stop
data: {"type":"content_block_stop","index":59               }

event: message_delta
data: {"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":184}  }

event: message_stop
data: {"type":"message_stop"          }

## [​](#important-considerations-when-using-extended-thinking) Important considerations when using extended thinking

**Working with the thinking budget:** The minimum budget is 1,024 tokens. We suggest starting at the minimum and increasing the thinking budget incrementally to find the optimal range for Claude to perform well for your use case. Higher token counts may allow you to achieve more comprehensive and nuanced reasoning, but there may also be diminishing returns depending on the task.

- The thinking budget is a target rather than a strict limit - actual token usage may vary based on the task.
- Be prepared for potentially longer response times due to the additional processing required for the reasoning process.
- Streaming is required when max_tokens is greater than 21,333.

**For thinking budgets above 32K:** We recommend using [batch processing](/en/docs/build-with-claude/batch-processing) for workloads where the thinking budget is set above 32K to avoid networking issues. Requests pushing the model to think above 32K tokens causes long running requests that might run up against system timeouts and open connection limits.

**Thinking compatibility with other features:**

- Thinking isn't compatible with temperature , top_p , or top_k modifications as well as [forced tool use](/en/docs/build-with-claude/tool-use#forcing-tool-use) .
- You cannot pre-fill responses when thinking is enabled.
- Changes to the thinking budget invalidate cached prompt prefixes that include messages. However, cached system prompts and tool definitions will continue to work when thinking parameters change.

### [​](#pricing-and-token-usage-for-extended-thinking) Pricing and token usage for extended thinking

Extended thinking tokens count towards the context window and are billed as output tokens. Since thinking tokens are treated as normal output tokens, they also count towards your rate limits. Be sure to account for this increased token usage when planning your API usage.

For Claude 3.7 Sonnet, the pricing is:

 Token use Cost

 Input tokens \$3 / MTok
 Output tokens (including thinking tokens) \$15 / MTok
 Prompt caching write \$3.75 / MTok
 Prompt caching read \$0.30 / MTok

[Batch processing](/en/docs/build-with-claude/batch-processing) for extended thinking is available at 50% off these prices and often completes in less than 1 hour.

All extended thinking tokens (including redacted thinking tokens) are billed as output tokens and count toward your rate limits.

In multi-turn conversations, thinking blocks associated with earlier assistant messages do not get billed as input tokens.

When extended thinking is enabled, a specialized 28 or 29 token system prompt is automatically included to support this feature.

Example: Previous thinking tokens omitted as input tokens for future turns

This example demonstrates that even though the second message includes the assistant's complete response with thinking blocks, the token counting API shows that previous thinking tokens don't contribute to the input token count for the subsequent turn:

```python
import anthropic

client = anthropic.Anthropic()

# First message with extended thinking enabled
first_message = [{
    "role": "user",
    "content": "Explain quantum entanglement"
}]

# Get the first response with extended thinking
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    messages=first_message
)

# Count tokens for the first exchange (just the user input)
first_count = client.messages.count_tokens(
    model="claude-3-7-sonnet-20250219",
    messages=first_message
)
print(f"First message input tokens: {first_count.input_tokens}")

# Prepare the second exchange with the previous response (including thinking blocks)
second_message = first_message + [
    {"role": "assistant", "content": response.content},
    {"role": "user", "content": "How does this relate to quantum computing?"}
]

# Count tokens for the second exchange
second_count = client.messages.count_tokens(
    model="claude-3-7-sonnet-20250219",
    messages=second_message
)
print(f"Second message input tokens: {second_count.input_tokens}")

# Extract text-only blocks to compare
text_only_blocks = [block for block in response.content if block.type == "text"]
text_only_content = [{"type": "text", "text": block.text} for block in text_only_blocks]

# Create a message with just the text blocks for comparison
text_only_message = first_message + [
    {"role": "assistant", "content": text_only_content},
    {"role": "user", "content": "How does this relate to quantum computing?"}
]

# Count tokens for this text-only message
text_only_count = client.messages.count_tokens(
    model="claude-3-7-sonnet-20250219",
    messages=text_only_message
)

# Compare token counts to prove previous thinking blocks aren't counted
print(f"Are they equal? {second_count.input_tokens == text_only_count.input_tokens}")
```

This behavior occurs because the Anthropic API automatically strips thinking blocks from previous turns when calculating context usage. This helps optimize token usage while maintaining the benefits of extended thinking.

### [​](#extended-output-capabilities-beta) Extended output capabilities (beta)

Claude 3.7 Sonnet can produce substantially longer responses than previous models with support for up to 128K output tokens (beta)---more than 15x longer than other Claude models. This expanded capability is particularly effective for extended thinking use cases involving complex reasoning, rich code generation, and comprehensive content creation.

This feature can be enabled by passing an anthropic-beta header of output-128k-2025-02-19 .

```python
import anthropic

client = anthropic.Anthropic()

with client.beta.messages.stream(
    model="claude-3-7-sonnet-20250219",
    max_tokens=128000,
    thinking={
        "type": "enabled",
        "budget_tokens": 32000
    },
    messages=[{
        "role": "user",
        "content": "Generate a comprehensive analysis of..."
    }],
    betas=["output-128k-2025-02-19"],
) as stream:
    for event in stream:
        if event.type == "content_block_start":
            print(f"\nStarting {event.content_block.type} block...")
        elif event.type == "content_block_delta":
            if event.delta.type == "thinking_delta":
                print(f"Thinking: {event.delta.thinking}", end="", flush=True)
            elif event.delta.type == "text_delta":
                print(f"Response: {event.delta.text}", end="", flush=True)
        elif event.type == "content_block_stop":
            print("\nBlock complete.")
```

When using extended thinking with longer outputs, you can allocate a larger thinking budget to support more thorough reasoning, while still having ample tokens available for the final response.

We suggest using streaming or batch mode with this extended output capability; for more details see our guidance on network reliability considerations for [long requests](/en/api/errors#long-requests) .

## [​](#using-extended-thinking-with-prompt-caching) Using extended thinking with prompt caching

Prompt caching with thinking has several important considerations:

**Thinking block inclusion in cached prompts**

- Thinking is only included when generating an assistant turn and not meant to be cached.
- Previous turn thinking blocks are ignored.
- If thinking becomes disabled, any thinking content passed to the API is simply ignored.

**Cache invalidation rules**

- Alterations to thinking parameters (enabling/disabling or budget changes) invalidate cache breakpoints set in messages.
- System prompts and tools maintain caching even when thinking parameters change.

### [​](#examples-of-prompt-caching-with-extended-thinking) Examples of prompt caching with extended thinking

System prompt caching (preserved when thinking changes)

```python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

SYSTEM_PROMPT=[
    {
        "type": "text",
        "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
    },
    {
        "type": "text",
        "text": LARGE_TEXT,
        "cache_control": {"type": "ephemeral"}
    }
]

MESSAGES = [
    {
        "role": "user",
        "content": "Analyze the tone of this passage."
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache hit expected because system prompt caching)
print("\nThird request - different thinking budget (cache hit expected)")
response3 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget - STILL maintains cache!
    },
    system=SYSTEM_PROMPT,
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

Here is the output of the script (you may see slightly different numbers)

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1365, cache_read_input_tokens: 0, input_tokens: 44, output_tokens: 725 }

Second request - same thinking parameters (cache hit expected)
Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1365, input_tokens: 386, output_tokens: 765 }

Third request - different thinking budget (cache hit expected)
Third response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1365, input_tokens: 811, output_tokens: 542 }
```

This example demonstrates that when caching is set up in the system prompt, changing the thinking parameters (budget_tokens increased from 4000 to 8000) **does not invalidate the cache** . The third request still shows a cache hit with cache_read_input_tokens=1365 , proving that system prompt caching is preserved even when thinking parameters change.

Messages caching (invalidated when thinking changes)

```python
from anthropic import Anthropic
import requests
from bs4 import BeautifulSoup

client = Anthropic()

def fetch_article_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text
    text = soup.get_text()

    # Break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # Break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # Drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text

# Fetch the content of the article
book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
book_content = fetch_article_content(book_url)
# Use just enough text for caching (first few chapters)
LARGE_TEXT = book_content[:5000]

# No system prompt - caching in messages instead
MESSAGES = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": LARGE_TEXT,
                "cache_control": {"type": "ephemeral"},
            },
            {
                "type": "text",
                "text": "Analyze the tone of this passage."
            }
        ]
    }
]

# First request - establish cache
print("First request - establishing cache")
response1 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000
    },
    messages=MESSAGES
)

print(f"First response usage: {response1.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response1.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the characters in this passage."
})
# Second request - same thinking parameters (cache hit expected)
print("\nSecond request - same thinking parameters (cache hit expected)")
response2 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 4000  # Same thinking budget
    },
    messages=MESSAGES
)

print(f"Second response usage: {response2.usage}")

MESSAGES.append({
    "role": "assistant",
    "content": response2.content
})
MESSAGES.append({
    "role": "user",
    "content": "Analyze the setting in this passage."
})

# Third request - different thinking budget (cache miss expected)
print("\nThird request - different thinking budget (cache miss expected)")
response3 = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 8000  # Different thinking budget breaks cache
    },
    messages=MESSAGES
)

print(f"Third response usage: {response3.usage}")
```

Here is the output of the script (you may see slightly different numbers)

```
First request - establishing cache
First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

Second request - same thinking parameters (cache hit expected)

Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

Third request - different thinking budget (cache miss expected)
Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
```

This example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget_tokens increased from 4000 to 8000) **invalidates the cache** . The third request shows no cache hit with cache_creation_input_tokens=1370 and cache_read_input_tokens=0 , proving that message-based caching is invalidated when thinking parameters change.

## [​](#max-tokens-and-context-window-size-with-extended-thinking) Max tokens and context window size with extended thinking

In older Claude models (prior to Claude 3.7 Sonnet), if the sum of prompt tokens and max_tokens exceeded the model's context window, the system would automatically adjust max_tokens to fit within the context limit. This meant you could set a large max_tokens value and the system would silently reduce it as needed.

With Claude 3.7 Sonnet, max_tokens (which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens + max_tokens exceeds the context window size.

### [​](#how-context-window-is-calculated-with-extended-thinking) How context window is calculated with extended thinking

When calculating context window usage with thinking enabled, there are some considerations to be aware of:

- Thinking blocks from previous turns are stripped and not counted towards your context window
- Current turn thinking counts towards your max_tokens limit for that turn

The diagram below demonstrates the specialized token management when extended thinking is enabled:

The effective context window is calculated as:

  context window = 
   (current input tokens - previous thinking tokens) + 
   (thinking tokens + redacted thinking tokens + text output tokens) 

We recommend using the [token counting API](/en/docs/build-with-claude/token-counting) to get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.

You can read through our [guide on context windows](/en/docs/context-windows) for a more thorough deep dive.

### [​](#managing-tokens-with-extended-thinking) Managing tokens with extended thinking

Given new context window and max_tokens behavior with extended thinking models like Claude 3.7 Sonnet, you may need to:

- More actively monitor and manage your token usage
- Adjust max_tokens values as your prompt length changes
- Potentially use the [token counting endpoints](/en/docs/build-with-claude/token-counting) more frequently
- Be aware that previous thinking blocks don't accumulate in your context window

This change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.

## [​](#extended-thinking-with-tool-use) Extended thinking with tool use

When using extended thinking with tool use, be aware of the following behavior pattern:

1. **First assistant turn** : When you send an initial user message, the assistant response will include thinking blocks followed by tool use requests.

2. **Tool result turn** : When you pass the user message with tool result blocks, **the subsequent assistant message will not contain any additional thinking blocks.**

To expand here, the normal order of a tool use conversation with thinking follows these steps:

1. User sends initial message
2. Assistant responds with thinking blocks and tool requests
3. User sends message with tool results
4. Assistant responds with either more tool calls or just text (no thinking blocks in this response)
5. If more tools are requested, repeat steps 3-4 until the conversation is complete

This design allows Claude to show its reasoning process before making tool requests, but not repeat the thinking process after receiving tool results. Claude will not output another thinking block until after the next non- tool_result user turn.

The diagram below illustrates the context window token management when combining extended thinking with tool use:

Example: Passing thinking blocks with tool results

Here's a practical example showing how to preserve thinking blocks when providing tool results:

```python
weather_tool = {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "input_schema": {
        "type": "object",
        "properties": {
            "location": {"type": "string"}
        },
        "required": ["location"]
    }
}

# First request - Claude responds with thinking and tool request
response = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"}
    ]
)
```

The API response will include thinking, text, and tool_use blocks:

```json
{
  "content": [
    {
      "type": "thinking",
      "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
      "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
    },
    {
      "type": "text",
      "text": "I can help you get the current weather information for Paris. Let me check that for you"
    },
    {
      "type": "tool_use",
      "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
      "name": "get_weather",
      "input": {
        "location": "Paris"
      }
    }
  ]
}
```

Now let's continue the conversation and use the tool

```python
# Extract thinking block and tool use block
thinking_block = next((block for block in response.content
                      if block.type == 'thinking'), None)
tool_use_block = next((block for block in response.content
                      if block.type == 'tool_use'), None)

# Call your actual weather API, here is where your actual API call would go
# let's pretend this is what we get back
weather_data = {"temperature": 88}

# Second request - Include thinking block and tool result
# No new thinking blocks will be generated in the response
continuation = client.messages.create(
    model="claude-3-7-sonnet-20250219",
    max_tokens=20000,
    thinking={
        "type": "enabled",
        "budget_tokens": 16000
    },
    tools=[weather_tool],
    messages=[
        {"role": "user", "content": "What's the weather in Paris?"},
        # notice that the thinking_block is passed in as well as the tool_use_block
        # if this is not passed in, an error is raised
        {"role": "assistant", "content": [thinking_block, tool_use_block]},
        {"role": "user", "content": [{
            "type": "tool_result",
            "tool_use_id": tool_use_block.id,
            "content": f"Current temperature: {weather_data['temperature']}°F"
        }]}
    ]
)
```

The API response will now **only** include text

```json
{
  "content": [
    {
      "type": "text",
      "text": "Currently in Paris, the temperature is 88°F (31°C)"
    }
  ]
}
```

### [​](#preserving-thinking-blocks) Preserving thinking blocks

During tool use, you must pass thinking and redacted_thinking blocks back to the API, and you must include the complete unmodified block back to the API. This is critical for maintaining the model's reasoning flow and conversation integrity.

While you can omit thinking and redacted_thinking blocks from prior assistant role turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:

- Automatically filter the provided thinking blocks
- Use the relevant thinking blocks necessary to preserve the model's reasoning
- Only bill for the input tokens for the blocks shown to Claude

#### [​](#why-thinking-blocks-must-be-preserved) Why thinking blocks must be preserved

When Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:

1. **Reasoning continuity** : The thinking blocks capture Claude's step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.

2. **Context maintenance** : While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls.

**Important** : When providing thinking or redacted_thinking blocks, the entire sequence of consecutive thinking or redacted_thinking blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.

## [​](#tips-for-making-the-best-use-of-extended-thinking-mode) Tips for making the best use of extended thinking mode

To get the most out of extended thinking:

1. **Set appropriate budgets** : Start with larger thinking budgets (16,000+ tokens) for complex tasks and adjust based on your needs.

2. **Experiment with thinking token budgets** : The model might perform differently at different max thinking budget settings. Increasing max thinking budget can make the model think better/harder, at the tradeoff of increased latency. For critical tasks, consider testing different budget settings to find the optimal balance between quality and performance.

3. **You do not need to remove previous thinking blocks yourself** : The Anthropic API automatically ignores thinking blocks from previous turns and they are not included when calculating context usage.

4. **Monitor token usage** : Keep track of thinking token usage to optimize costs and performance.

5. **Use extended thinking for particularly complex tasks** : Enable thinking for tasks that benefit from step-by-step reasoning like math, coding, and analysis.

6. **Account for extended response time** : Factor in that generating thinking blocks may increase overall response time.

7. **Handle streaming appropriately** : When streaming, be prepared to handle both thinking and text content blocks as they arrive.

8. **Prompt engineering** : Review our [extended thinking prompting tips](/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips) if you want to maximize Claude's thinking capabilities.

## [​](#next-steps) Next steps

(https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking)

## Try the extended thinking cookbook

Explore practical examples of thinking in our cookbook.

(/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips)

## Extended thinking prompting tips

Learn prompt engineering best practices for extended thinking.
