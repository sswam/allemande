# 1. Front
What is a computational model inspired by the human brain, consisting of interconnected nodes organized in layers?

# 1. Back
A **Neural Network**

# 1. Extra
Neural networks are the foundation of deep learning. They consist of:
- **Input layer**: Receives initial data
- **Hidden layer(s)**: Processes information
- **Output layer**: Produces final results

Key features:
- **Nodes** (neurons): Basic computational units
- **Weights**: Strength of connections between nodes
- **Bias**: Additional parameter to adjust the output

Diagram:
```mermaid
graph LR
    A[Input Layer] --> B[Hidden Layer]
    B --> C[Output Layer]
```

Applications:
- Image and speech recognition
- Natural language processing
- Autonomous vehicles
- Game playing (e.g., AlphaGo)

```python
# Simple neural network in PyTorch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(10, 5)
        self.layer2 = nn.Linear(5, 1)
    
    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x
```


# 2. Front
What subset of machine learning uses multi-layered neural networks to learn hierarchical representations of data?

# 2. Back
**Deep Learning**

# 2. Extra
Deep Learning is characterized by:
- Multiple layers of neural networks (typically more than 3)
- Automatic feature extraction
- Ability to handle large amounts of unstructured data

Key concepts:
- **Backpropagation**: Algorithm for training neural networks
- **Gradient descent**: Optimization technique for adjusting weights
- **Overfitting**: When a model learns noise in training data

Popular deep learning architectures:
1. Convolutional Neural Networks (CNNs)
2. Recurrent Neural Networks (RNNs)
3. Long Short-Term Memory (LSTM)
4. Transformers

Frameworks:
- TensorFlow
- PyTorch
- Keras

Diagram of deep learning in the AI hierarchy:
```mermaid
graph TD
    A[Artificial Intelligence] --> B[Machine Learning]
    B --> C[Deep Learning]
    C --> D[Neural Networks]
    D --> E[CNNs]
    D --> F[RNNs]
    D --> G[Transformers]
```

```python
# Deep learning model in TensorFlow
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```


# 3. Front
What is the simplest form of a neural network, consisting of a single artificial neuron with weighted inputs and an activation function?

# 3. Back
A **Perceptron**

# 3. Extra
The perceptron is a fundamental building block of neural networks, introduced by Frank Rosenblatt in 1958.

Key components:
1. **Inputs**: $x_1, x_2, ..., x_n$
2. **Weights**: $w_1, w_2, ..., w_n$
3. **Bias**: $b$
4. **Activation function**: $f$

Mathematical representation:
$y = f(\sum_{i=1}^n w_i x_i + b)$

Common activation functions for perceptrons:
- Step function
- Sign function

Limitations:
- Can only learn linearly separable patterns
- Cannot solve XOR problem

Diagram:
```mermaid
graph LR
    A[x1] --> D((Σ))
    B[x2] --> D
    C[xn] --> D
    D --> E{f}
    E --> F[Output]
    G[Bias] --> D
```

Python implementation:
```python
import numpy as np

class Perceptron:
    def __init__(self, input_size):
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()
    
    def activate(self, x):
        return 1 if x > 0 else 0
    
    def predict(self, inputs):
        sum_inputs = np.dot(inputs, self.weights) + self.bias
        return self.activate(sum_inputs)
```

Historical significance:
The perceptron paved the way for more complex neural network architectures and deep learning models.


# 4. Front
What mathematical function determines the output of a neural network node based on its input?

# 4. Back
An **Activation Function**

# 4. Extra
Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.

Common activation functions:
1. **ReLU** (Rectified Linear Unit): $f(x) = max(0, x)$
2. **Sigmoid**: $f(x) = \frac{1}{1 + e^{-x}}$
3. **Tanh**: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
4. **Leaky ReLU**: $f(x) = max(0.01x, x)$
5. **Softmax**: $f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$

Characteristics to consider:
- Range
- Differentiability
- Monotonicity
- Computational efficiency

Visualization of common activation functions:
![Activation Functions](https://miro.medium.com/max/1400/1*p_hyqAtyI8pbt2kEl6siOQ.png)

Usage in Python (TensorFlow):
```python
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='tanh'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

Choosing the right activation function:
- ReLU is often the default choice for hidden layers
- Sigmoid or tanh for binary classification output
- Softmax for multi-class classification output

Vanishing/Exploding gradient problem:
Some activation functions (e.g., sigmoid, tanh) can lead to vanishing or exploding gradients in deep networks, making training difficult.


# 5. Front
What S-shaped activation function maps input values to a range between 0 and 1?

# 5. Back
The **Sigmoid Function**

# 5. Extra
The sigmoid function, also known as the logistic function, is defined as:

$f(x) = \frac{1}{1 + e^{-x}}$

Key properties:
- Output range: (0, 1)
- Smooth and continuous
- Easily differentiable

Derivative:
$f'(x) = f(x)(1 - f(x))$

Graph of the sigmoid function:
```
1 │    -------
  │   /
  │  /
  │ /
0 │/
  └─────────────
   -∞    0    +∞
```

Use cases:
- Binary classification
- Output layer of neural networks
- Logistic regression

Limitations:
- Vanishing gradient problem for deep networks
- Not zero-centered

Python implementation:
```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title("Sigmoid Function")
plt.xlabel("x")
plt.ylabel("sigmoid(x)")
plt.grid(True)
plt.show()
```

Historical significance:
The sigmoid function was widely used in early neural networks but has been largely replaced by ReLU in hidden layers of deep networks due to the vanishing gradient problem.



# 6. Front
What activation function outputs the input directly if positive, otherwise outputs zero?

# 6. Back
The **Rectified Linear Unit (ReLU)** activation function

# 6. Extra
ReLU is defined mathematically as:

$f(x) = \max(0, x)$

It helps mitigate the **vanishing gradient problem** by allowing gradients to flow through the network more easily for positive inputs. ReLU is computationally efficient and has become one of the most popular activation functions in deep learning.

Code example in Python:
```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

# Usage
input_array = np.array([-2, -1, 0, 1, 2])
output_array = relu(input_array)
print(output_array)  # [0 0 0 1 2]
```

Advantages of ReLU:
1. Non-linear activation
2. Sparse activation (many neurons output 0)
3. Efficient computation
4. Reduces likelihood of vanishing gradient

Disadvantages:
1. "Dying ReLU" problem for negative inputs
2. Not zero-centered

Variants:
- Leaky ReLU: $f(x) = \max(αx, x)$, where $α$ is a small positive constant
- Parametric ReLU (PReLU): Learnable $α$ parameter
- Exponential Linear Unit (ELU): $f(x) = x$ if $x > 0$, $α(e^x - 1)$ otherwise

<img src="https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png" alt="ReLU function graph" width="400"/>


# 7. Front
What algorithm is used to calculate gradients in neural networks by propagating errors backward through the network?

# 7. Back
**Backpropagation**

# 7. Extra
Backpropagation, short for "backward propagation of errors," is a fundamental algorithm in training neural networks. It efficiently computes the gradient of the loss function with respect to the weights of the network.

Key concepts:
1. **Forward pass**: Compute the output of the network given input data
2. **Compute loss**: Calculate the error between the predicted output and the true target
3. **Backward pass**: Propagate the error backwards through the network, computing gradients
4. **Update weights**: Adjust the weights using an optimization algorithm (e.g., gradient descent)

Mathematical representation:
For a weight $w_{ij}$ connecting neuron $i$ to neuron $j$, the gradient is computed as:

$\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}$

Where:
- $E$ is the error (loss)
- $a_j$ is the activation of neuron $j$
- $z_j$ is the weighted sum of inputs to neuron $j$

Pseudocode for backpropagation:
```
1. Initialize network weights randomly
2. For each training example:
   a. Forward pass: Compute activations for all neurons
   b. Compute loss
   c. Backward pass:
      - Compute output layer gradients
      - For each hidden layer (from last to first):
        - Compute gradients
   d. Update weights using computed gradients
3. Repeat steps 2-3 for multiple epochs
```

Advantages:
- Efficient computation of gradients
- Enables training of deep neural networks
- Applicable to various network architectures

Challenges:
- Vanishing/exploding gradients in deep networks
- Local minima and saddle points
- Computationally intensive for large networks

Variants and improvements:
- Stochastic gradient descent (SGD)
- Mini-batch gradient descent
- Momentum
- Adam optimizer

<img src="https://miro.medium.com/max/1400/1*3WNIuH7XE3gN7wQxZFLlmA.png" alt="Backpropagation illustration" width="500"/>

