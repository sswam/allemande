Liquid AI Introduces Liquid Foundation Models (LFMs): A 1B, 3B, and 40B Series of Generative AI Models

Liquid AI has released its first series of Liquid Foundation Models (LFMs), including 1B, 3B, and 40B parameter configurations. Key points:

- LFM-1B (1 billion parameters) achieves top benchmark scores for its size class
- LFM-3B (3 billion parameters) outperforms competitors and rivals larger 7B-13B models
- LFM-40B (40 billion parameters) uses Mixture of Experts architecture for efficiency

The models leverage computational units based on dynamical systems, signal processing, and linear algebra theories. They excel at general/expert knowledge, math/logic, and long-context tasks. Strengths include multilingual capabilities and efficient processing of long contexts up to 32k tokens.

LFMs are optimized for various hardware platforms and currently available on several deployment options. While not open-source, they represent an innovative approach aiming to redefine AI model design and deployment through superior performance and efficiency.
